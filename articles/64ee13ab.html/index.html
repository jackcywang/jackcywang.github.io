<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"wangng.com",root:"/",scheme:"Pisces",version:"7.7.2",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="1.vggNet简介vgg16是2014年由牛津大学提出的一个深度神经网络模型，该模型在2014年的ILSVRC分类比赛中，取得了第二名的成绩，而第一名当属大名鼎鼎的googleNet,vggNet包含5种网络类型，如下图所示："><meta property="og:type" content="article"><meta property="og:title" content="基于vgg16的迁移学习,训练自己的数据集(含预测结果)"><meta property="og:url" content="https://wangng.com/articles/64ee13ab.html/index.html"><meta property="og:site_name" content="jackcywang&#39;s blog"><meta property="og:description" content="1.vggNet简介vgg16是2014年由牛津大学提出的一个深度神经网络模型，该模型在2014年的ILSVRC分类比赛中，取得了第二名的成绩，而第一名当属大名鼎鼎的googleNet,vggNet包含5种网络类型，如下图所示："><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://cdn.wangng.com/Deeplearning/%E5%9F%BA%E4%BA%8Evgg16%E7%9A%84%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%90%AB%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C/vgg16.png"><meta property="og:image" content="http://cdn.wangng.com/Deeplearning/%E5%9F%BA%E4%BA%8Evgg16%E7%9A%84%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%90%AB%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C/5x5.png"><meta property="article:published_time" content="2019-05-18T14:21:44.000Z"><meta property="article:modified_time" content="2020-05-01T15:08:07.805Z"><meta property="article:author" content="jackcywang"><meta property="article:tag" content="Deeplearning"><meta property="article:tag" content="Classification"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://cdn.wangng.com/Deeplearning/%E5%9F%BA%E4%BA%8Evgg16%E7%9A%84%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%90%AB%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C/vgg16.png"><link rel="canonical" href="https://wangng.com/articles/64ee13ab.html/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>基于vgg16的迁移学习,训练自己的数据集(含预测结果) | jackcywang's blog</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?8ce866754ae6c95a7238894d36ad3ab0";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div> <a href="https://github.com/jackcywang" target="_blank" rel="noopener" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64ceaa;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">jackcywang's blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">梦想与现实只在一念之间</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li></ul></nav></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://wangng.com/articles/64ee13ab.html/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="jackcywang"><meta itemprop="description" content="不经一番彻骨寒,怎得梅花扑鼻香"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="jackcywang's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 基于vgg16的迁移学习,训练自己的数据集(含预测结果)</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-18 22:21:44" itemprop="dateCreated datePublished" datetime="2019-05-18T22:21:44+08:00">2019-05-18</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deeplearning/" itemprop="url" rel="index"><span itemprop="name">Deeplearning</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deeplearning/Classification/" itemprop="url" rel="index"><span itemprop="name">Classification</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/articles/64ee13ab.html/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/articles/64ee13ab.html/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>24k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>22 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-vggNet简介"><a href="#1-vggNet简介" class="headerlink" title="1.vggNet简介"></a>1.vggNet简介</h2><p>vgg16是2014年由牛津大学提出的一个深度神经网络模型，该模型在2014年的ILSVRC分类比赛中，取得了第二名的成绩，而第一名当属大名鼎鼎的googleNet,vggNet包含5种网络类型，如下图所示：</p><a id="more"></a><p><img src="http://cdn.wangng.com/Deeplearning/基于vgg16的迁移学习-训练自己的数据集-含预测结果/vgg16.png" alt="vgg16"><br>常见的有vgg16和vgg19。顾名思义vgg16有16层，包含13层卷积池化层和3层全连接层。而vgg19包含16层卷积池化层和3层全连接层。vggNet全部使用1x1,3x3的卷积核，而且vggNet证明了两个3x3的卷积核可以等效为一个5x5的卷积核，下图示<br><img src="http://cdn.wangng.com/Deeplearning/基于vgg16的迁移学习-训练自己的数据集-含预测结果/5x5.png" alt="5x5"><br>一张5x5的图经两个3x3的卷积核卷积后得到一张1x1的特征图，等效为一个5x5的卷积核。同时在参数量上可以发现，5x5的卷积核的参数量是5x5=25，两个3x3的卷积核是2x3x3=18,参数量是减少了的28%，同时由于与一个5x5的卷积核卷积只需一次非线性激活，而与两个卷积核卷积可以进行两次非线性激活变换，非线性表征加强了，增加了CNN对特征的学习能力。另外1x1卷积核能实现降维，增加非线性。</p><h2 id="2-vgg16实现迁移学习"><a href="#2-vgg16实现迁移学习" class="headerlink" title="2.vgg16实现迁移学习"></a>2.vgg16实现迁移学习</h2><p>1.数据集准备，我使用8类数据，分别是truck,tiger,flower,kittycat,guitar,houses,plane,person,数据每类训练集500张，验证集300张</p><p>2.vgg16预训练权重下载，我把它放在我的<a href="https://pan.baidu.com/s/1zasR4Vgw2awn2O1V2X9g-A" target="_blank" rel="noopener">百度网盘</a>里了，密码fwi4</p><p>3.生成train.txt,val.txt,label.txt<br><em>create_labels_files.py</em><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"># -*-coding:utf-8-*-</span><br><span class="line"> </span><br><span class="line">import os</span><br><span class="line">import os.path</span><br><span class="line"> </span><br><span class="line">def write_txt(content, filename, mode&#x3D;&#39;w&#39;):</span><br><span class="line">    &quot;&quot;&quot;保存txt数据</span><br><span class="line">    :param content:需要保存的数据,type-&gt;list</span><br><span class="line">    :param filename:文件名</span><br><span class="line">    :param mode:读写模式:&#39;w&#39; or &#39;a&#39;</span><br><span class="line">    :return: void</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    with open(filename, mode) as f:</span><br><span class="line">        for line in content:</span><br><span class="line">            str_line &#x3D; &quot;&quot;</span><br><span class="line">            for col, data in enumerate(line):</span><br><span class="line">                if not col &#x3D;&#x3D; len(line) - 1:</span><br><span class="line">                    # 以空格作为分隔符</span><br><span class="line">                    str_line &#x3D; str_line + str(data) + &quot; &quot;</span><br><span class="line">                else:</span><br><span class="line">                    # 每行最后一个数据用换行符“\n”</span><br><span class="line">                    str_line &#x3D; str_line + str(data) + &quot;\n&quot;</span><br><span class="line">            f.write(str_line)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def get_files_list(dir):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    实现遍历dir目录下,所有文件(包含子文件夹的文件)</span><br><span class="line">    :param dir:指定文件夹目录</span><br><span class="line">    :return:包含所有文件的列表-&gt;list</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # parent:父目录, filenames:该目录下所有文件夹,filenames:该目录下的文件名</span><br><span class="line">    files_list &#x3D; []</span><br><span class="line">    for parent, dirnames, filenames in os.walk(dir):</span><br><span class="line">        for filename in filenames:</span><br><span class="line">            print(&quot;parent is: &quot; + parent)</span><br><span class="line">            print(&quot;filename is: &quot; + filename)</span><br><span class="line">            # print(os.path.join(parent, filename))  # 输出rootdir路径下所有文件（包含子文件）信息</span><br><span class="line">            curr_file &#x3D; parent.split(os.sep)[-1]</span><br><span class="line">            if curr_file &#x3D;&#x3D; &#39;flower&#39;:</span><br><span class="line">                labels &#x3D; 0</span><br><span class="line">            elif curr_file &#x3D;&#x3D; &#39;guitar&#39;:</span><br><span class="line">                labels &#x3D; 1</span><br><span class="line">            elif curr_file &#x3D;&#x3D; &#39;person&#39;:</span><br><span class="line">                labels &#x3D; 2</span><br><span class="line">            elif curr_file &#x3D;&#x3D; &#39;houses&#39;:</span><br><span class="line">                labels &#x3D; 3</span><br><span class="line">            elif curr_file &#x3D;&#x3D; &#39;plane&#39;:</span><br><span class="line">                labels &#x3D; 4</span><br><span class="line">            elif curr_file &#x3D;&#x3D; &#39;tiger&#39;:</span><br><span class="line">                labels &#x3D; 5</span><br><span class="line">            elif curr_file &#x3D;&#x3D; &#39;kittycat&#39;:</span><br><span class="line">                labels &#x3D; 6</span><br><span class="line">            elif curr_file &#x3D;&#x3D; &#39;truck&#39;:</span><br><span class="line">                labels &#x3D; 7</span><br><span class="line">            files_list.append([os.path.join(curr_file, filename), labels])</span><br><span class="line">            print(files_list)</span><br><span class="line">    return files_list</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    train_dir &#x3D; &#39;dataset&#x2F;train&#39;</span><br><span class="line">    train_txt &#x3D; &#39;dataset&#x2F;train.txt&#39;</span><br><span class="line">    train_data &#x3D; get_files_list(train_dir)</span><br><span class="line">    write_txt(train_data, train_txt, mode&#x3D;&#39;w&#39;)</span><br><span class="line"> </span><br><span class="line">    val_dir &#x3D; &#39;dataset&#x2F;val&#39;</span><br><span class="line">    val_txt &#x3D; &#39;dataset&#x2F;val.txt&#39;</span><br><span class="line">    val_data &#x3D; get_files_list(val_dir)</span><br><span class="line">    write_txt(val_data, val_txt, mode&#x3D;&#39;w&#39;)</span><br></pre></td></tr></table></figure><br>4.制作tf.record文件<br><em>create_tf_record.py</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br></pre></td><td class="code"><pre><span class="line"># -*-coding: utf-8 -*-</span><br><span class="line"> </span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">from PIL import Image</span><br><span class="line"> </span><br><span class="line">def _int64_feature(value):</span><br><span class="line">    return tf.train.Feature(int64_list&#x3D;tf.train.Int64List(value&#x3D;[value]))</span><br><span class="line"># 生成字符串型的属性</span><br><span class="line">def _bytes_feature(value):</span><br><span class="line">    return tf.train.Feature(bytes_list&#x3D;tf.train.BytesList(value&#x3D;[value]))</span><br><span class="line"># 生成实数型的属性</span><br><span class="line">def float_list_feature(value):</span><br><span class="line">  return tf.train.Feature(float_list&#x3D;tf.train.FloatList(value&#x3D;value))</span><br><span class="line"> </span><br><span class="line">def get_example_nums(tf_records_filenames):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    统计tf_records图像的个数(example)个数</span><br><span class="line">    :param tf_records_filenames: tf_records文件路径</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    nums&#x3D; 0</span><br><span class="line">    for record in tf.python_io.tf_record_iterator(tf_records_filenames):</span><br><span class="line">        nums +&#x3D; 1</span><br><span class="line">    return nums</span><br><span class="line"> </span><br><span class="line">def show_image(title,image):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    显示图片</span><br><span class="line">    :param title: 图像标题</span><br><span class="line">    :param image: 图像的数据</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # plt.figure(&quot;show_image&quot;)</span><br><span class="line">    # print(image.dtype)</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.axis(&#39;on&#39;)    # 关掉坐标轴为 off</span><br><span class="line">    plt.title(title)  # 图像题目</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line">def load_labels_file(filename,labels_num&#x3D;1,shuffle&#x3D;False):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    载图txt文件，文件中每行为一个图片信息，且以空格隔开：图像路径 标签1 标签2，如：test_image&#x2F;1.jpg 0 2</span><br><span class="line">    :param filename:</span><br><span class="line">    :param labels_num :labels个数</span><br><span class="line">    :param shuffle :是否打乱顺序</span><br><span class="line">    :return:images type-&gt;list</span><br><span class="line">    :return:labels type-&gt;list</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    images&#x3D;[]</span><br><span class="line">    labels&#x3D;[]</span><br><span class="line">    with open(filename) as f:</span><br><span class="line">        lines_list&#x3D;f.readlines()</span><br><span class="line">        if shuffle:</span><br><span class="line">            random.shuffle(lines_list)</span><br><span class="line"> </span><br><span class="line">        for lines in lines_list:</span><br><span class="line">            line&#x3D;lines.rstrip().split(&#39; &#39;)</span><br><span class="line">            label&#x3D;[]</span><br><span class="line">            for i in range(labels_num):</span><br><span class="line">                label.append(int(line[i+1]))</span><br><span class="line">            images.append(line[0])</span><br><span class="line">            labels.append(label)</span><br><span class="line">    return images,labels</span><br><span class="line"> </span><br><span class="line">def read_image(filename, resize_height, resize_width,normalization&#x3D;False):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    读取图片数据,默认返回的是uint8,[0,255]</span><br><span class="line">    :param filename:</span><br><span class="line">    :param resize_height:</span><br><span class="line">    :param resize_width:</span><br><span class="line">    :param normalization:是否归一化到[0.,1.0]</span><br><span class="line">    :return: 返回的图片数据</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line"> </span><br><span class="line">    bgr_image &#x3D; cv2.imread(filename)</span><br><span class="line">    if None is bgr_image:</span><br><span class="line">        pass</span><br><span class="line">    elif len(bgr_image.shape)&#x3D;&#x3D;2:#若是灰度图则转为三通道</span><br><span class="line">        print(&quot;Warning:gray image&quot;,filename)</span><br><span class="line">        bgr_image &#x3D; cv2.cvtColor(bgr_image, cv2.COLOR_GRAY2BGR)</span><br><span class="line">    print(filename)</span><br><span class="line">    rgb_image &#x3D; cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)#将BGR转为RGB</span><br><span class="line">    # show_image(filename,rgb_image)</span><br><span class="line">    # rgb_image&#x3D;Image.open(filename)</span><br><span class="line">    if resize_height&gt;0 and resize_width&gt;0:</span><br><span class="line">        rgb_image&#x3D;cv2.resize(rgb_image,(resize_width,resize_height))</span><br><span class="line">    rgb_image&#x3D;np.asanyarray(rgb_image)</span><br><span class="line">    if normalization:</span><br><span class="line">        # 不能写成:rgb_image&#x3D;rgb_image&#x2F;255</span><br><span class="line">        rgb_image&#x3D;rgb_image&#x2F;255.0</span><br><span class="line">    # show_image(&quot;src resize image&quot;,image)</span><br><span class="line">    return rgb_image</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def get_batch_images(images,labels,batch_size,labels_nums,one_hot&#x3D;False,shuffle&#x3D;True,num_threads&#x3D;1):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    :param images:图像</span><br><span class="line">    :param labels:标签</span><br><span class="line">    :param batch_size:</span><br><span class="line">    :param labels_nums:标签个数</span><br><span class="line">    :param one_hot:是否将labels转为one_hot的形式</span><br><span class="line">    :param shuffle:是否打乱顺序,一般train时shuffle&#x3D;True,验证时shuffle&#x3D;False</span><br><span class="line">    :return:返回batch的images和labels</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    min_after_dequeue &#x3D; 200</span><br><span class="line">    capacity &#x3D; min_after_dequeue + 3 * batch_size  # 保证capacity必须大于min_after_dequeue参数值</span><br><span class="line">    if shuffle:</span><br><span class="line">        images_batch, labels_batch &#x3D; tf.train.shuffle_batch([images,labels],</span><br><span class="line">                                                                    batch_size&#x3D;batch_size,</span><br><span class="line">                                                                    capacity&#x3D;capacity,</span><br><span class="line">                                                                    min_after_dequeue&#x3D;min_after_dequeue,</span><br><span class="line">                                                                    num_threads&#x3D;num_threads)</span><br><span class="line">    else:</span><br><span class="line">        images_batch, labels_batch &#x3D; tf.train.batch([images,labels],</span><br><span class="line">                                                        batch_size&#x3D;batch_size,</span><br><span class="line">                                                        capacity&#x3D;capacity,</span><br><span class="line">                                                        num_threads&#x3D;num_threads)</span><br><span class="line">    if one_hot:</span><br><span class="line">        labels_batch &#x3D; tf.one_hot(labels_batch, labels_nums, 1, 0)</span><br><span class="line">    return images_batch,labels_batch</span><br><span class="line"> </span><br><span class="line">def read_records(filename,resize_height, resize_width,type&#x3D;None):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    解析record文件:源文件的图像数据是RGB,uint8,[0,255],一般作为训练数据时,需要归一化到[0,1]</span><br><span class="line">    :param filename:</span><br><span class="line">    :param resize_height:</span><br><span class="line">    :param resize_width:</span><br><span class="line">    :param type:选择图像数据的返回类型</span><br><span class="line">         None:默认将uint8-[0,255]转为float32-[0,255]</span><br><span class="line">         normalization:归一化float32-[0,1]</span><br><span class="line">         centralization:归一化float32-[0,1],再减均值中心化</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # 创建文件队列,不限读取的数量</span><br><span class="line">    filename_queue &#x3D; tf.train.string_input_producer([filename])</span><br><span class="line">    # create a reader from file queue</span><br><span class="line">    reader &#x3D; tf.TFRecordReader()</span><br><span class="line">    # reader从文件队列中读入一个序列化的样本</span><br><span class="line">    _, serialized_example &#x3D; reader.read(filename_queue)</span><br><span class="line">    # get feature from serialized example</span><br><span class="line">    # 解析符号化的样本</span><br><span class="line">    features &#x3D; tf.parse_single_example(</span><br><span class="line">        serialized_example,</span><br><span class="line">        features&#x3D;&#123;</span><br><span class="line">            &#39;image_raw&#39;: tf.FixedLenFeature([], tf.string),</span><br><span class="line">            &#39;height&#39;: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">            &#39;width&#39;: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">            &#39;depth&#39;: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">            &#39;label&#39;: tf.FixedLenFeature([], tf.int64)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    tf_image &#x3D; tf.decode_raw(features[&#39;image_raw&#39;], tf.uint8)#获得图像原始的数据</span><br><span class="line"> </span><br><span class="line">    tf_height &#x3D; features[&#39;height&#39;]</span><br><span class="line">    tf_width &#x3D; features[&#39;width&#39;]</span><br><span class="line">    tf_depth &#x3D; features[&#39;depth&#39;]</span><br><span class="line">    tf_label &#x3D; tf.cast(features[&#39;label&#39;], tf.int32)</span><br><span class="line">    # PS:恢复原始图像数据,reshape的大小必须与保存之前的图像shape一致,否则出错</span><br><span class="line">    # tf_image&#x3D;tf.reshape(tf_image, [-1])    # 转换为行向量</span><br><span class="line">    tf_image&#x3D;tf.reshape(tf_image, [resize_height, resize_width, 3]) # 设置图像的维度</span><br><span class="line"> </span><br><span class="line">    # 恢复数据后,才可以对图像进行resize_images:输入uint-&gt;输出float32</span><br><span class="line">    # tf_image&#x3D;tf.image.resize_images(tf_image,[224, 224])</span><br><span class="line"> </span><br><span class="line">    # 存储的图像类型为uint8,tensorflow训练时数据必须是tf.float32</span><br><span class="line">    if type is None:</span><br><span class="line">        tf_image &#x3D; tf.cast(tf_image, tf.float32)</span><br><span class="line">    elif type&#x3D;&#x3D;&#39;normalization&#39;:# [1]若需要归一化请使用:</span><br><span class="line">        # 仅当输入数据是uint8,才会归一化[0,255]</span><br><span class="line">        # tf_image &#x3D; tf.image.convert_image_dtype(tf_image, tf.float32)</span><br><span class="line">        tf_image &#x3D; tf.cast(tf_image, tf.float32) * (1. &#x2F; 255.0)  # 归一化</span><br><span class="line">    elif type&#x3D;&#x3D;&#39;centralization&#39;:</span><br><span class="line">        # 若需要归一化,且中心化,假设均值为0.5,请使用:</span><br><span class="line">        tf_image &#x3D; tf.cast(tf_image, tf.float32) * (1. &#x2F; 255) - 0.5 #中心化</span><br><span class="line"> </span><br><span class="line">    # 这里仅仅返回图像和标签</span><br><span class="line">    # return tf_image, tf_height,tf_width,tf_depth,tf_label</span><br><span class="line">    return tf_image,tf_label</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def create_records(image_dir,file, output_record_dir, resize_height, resize_width,shuffle,log&#x3D;5):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    实现将图像原始数据,label,长,宽等信息保存为record文件</span><br><span class="line">    注意:读取的图像数据默认是uint8,再转为tf的字符串型BytesList保存,解析请需要根据需要转换类型</span><br><span class="line">    :param image_dir:原始图像的目录</span><br><span class="line">    :param file:输入保存图片信息的txt文件(image_dir+file构成图片的路径)</span><br><span class="line">    :param output_record_dir:保存record文件的路径</span><br><span class="line">    :param resize_height:</span><br><span class="line">    :param resize_width:</span><br><span class="line">    PS:当resize_height或者resize_width&#x3D;0是,不执行resize</span><br><span class="line">    :param shuffle:是否打乱顺序</span><br><span class="line">    :param log:log信息打印间隔</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # 加载文件,仅获取一个label</span><br><span class="line">    images_list, labels_list&#x3D;load_labels_file(file,1,shuffle)</span><br><span class="line"> </span><br><span class="line">    writer &#x3D; tf.python_io.TFRecordWriter(output_record_dir)</span><br><span class="line">    for i, [image_name, labels] in enumerate(zip(images_list, labels_list)):</span><br><span class="line">        image_path&#x3D;os.path.join(image_dir,images_list[i])</span><br><span class="line">        if not os.path.exists(image_path):</span><br><span class="line">            print(&#39;Err:no image&#39;,image_path)</span><br><span class="line">            continue</span><br><span class="line">        image &#x3D; read_image(image_path, resize_height, resize_width)</span><br><span class="line">        image_raw &#x3D; image.tostring()</span><br><span class="line">        if i%log&#x3D;&#x3D;0 or i&#x3D;&#x3D;len(images_list)-1:</span><br><span class="line">            print(&#39;------------processing:%d-th------------&#39; % (i))</span><br><span class="line">            print(&#39;current image_path&#x3D;%s&#39; % (image_path),&#39;shape:&#123;&#125;&#39;.format(image.shape),&#39;labels:&#123;&#125;&#39;.format(labels))</span><br><span class="line">        # 这里仅保存一个label,多label适当增加&quot;&#39;label&#39;: _int64_feature(label)&quot;项</span><br><span class="line">        label&#x3D;labels[0]</span><br><span class="line">        example &#x3D; tf.train.Example(features&#x3D;tf.train.Features(feature&#x3D;&#123;</span><br><span class="line">            &#39;image_raw&#39;: _bytes_feature(image_raw),</span><br><span class="line">            &#39;height&#39;: _int64_feature(image.shape[0]),</span><br><span class="line">            &#39;width&#39;: _int64_feature(image.shape[1]),</span><br><span class="line">            &#39;depth&#39;: _int64_feature(image.shape[2]),</span><br><span class="line">            &#39;label&#39;: _int64_feature(label)</span><br><span class="line">        &#125;))</span><br><span class="line">        writer.write(example.SerializeToString())</span><br><span class="line">    writer.close()</span><br><span class="line"> </span><br><span class="line">def disp_records(record_file,resize_height, resize_width,show_nums&#x3D;4):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    解析record文件，并显示show_nums张图片，主要用于验证生成record文件是否成功</span><br><span class="line">    :param tfrecord_file: record文件路径</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # 读取record函数</span><br><span class="line">    tf_image, tf_label &#x3D; read_records(record_file,resize_height,resize_width,type&#x3D;&#39;normalization&#39;)</span><br><span class="line">    # 显示前4个图片</span><br><span class="line">    init_op &#x3D; tf.initialize_all_variables()</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        coord &#x3D; tf.train.Coordinator()</span><br><span class="line">        threads &#x3D; tf.train.start_queue_runners(sess&#x3D;sess, coord&#x3D;coord)</span><br><span class="line">        for i in range(show_nums):</span><br><span class="line">            image,label &#x3D; sess.run([tf_image,tf_label])  # 在会话中取出image和label</span><br><span class="line">            # image &#x3D; tf_image.eval()</span><br><span class="line">            # 直接从record解析的image是一个向量,需要reshape显示</span><br><span class="line">            # image &#x3D; image.reshape([height,width,depth])</span><br><span class="line">            print(&#39;shape:&#123;&#125;,tpye:&#123;&#125;,labels:&#123;&#125;&#39;.format(image.shape,image.dtype,label))</span><br><span class="line">            # pilimg &#x3D; Image.fromarray(np.asarray(image_eval_reshape))</span><br><span class="line">            # pilimg.show()</span><br><span class="line">            show_image(&quot;image:%d&quot;%(label),image)</span><br><span class="line">        coord.request_stop()</span><br><span class="line">        coord.join(threads)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def batch_test(record_file,resize_height, resize_width):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    :param record_file: record文件路径</span><br><span class="line">    :param resize_height:</span><br><span class="line">    :param resize_width:</span><br><span class="line">    :return:</span><br><span class="line">    :PS:image_batch, label_batch一般作为网络的输入</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # 读取record函数</span><br><span class="line">    tf_image,tf_label &#x3D; read_records(record_file,resize_height,resize_width,type&#x3D;&#39;normalization&#39;)</span><br><span class="line">    image_batch, label_batch&#x3D; get_batch_images(tf_image,tf_label,batch_size&#x3D;4,labels_nums&#x3D;5,one_hot&#x3D;False,shuffle&#x3D;False)</span><br><span class="line"> </span><br><span class="line">    init &#x3D; tf.global_variables_initializer()</span><br><span class="line">    with tf.Session() as sess:  # 开始一个会话</span><br><span class="line">        sess.run(init)</span><br><span class="line">        coord &#x3D; tf.train.Coordinator()</span><br><span class="line">        threads &#x3D; tf.train.start_queue_runners(coord&#x3D;coord)</span><br><span class="line">        for i in range(4):</span><br><span class="line">            # 在会话中取出images和labels</span><br><span class="line">            images, labels &#x3D; sess.run([image_batch, label_batch])</span><br><span class="line">            # 这里仅显示每个batch里第一张图片</span><br><span class="line">            show_image(&quot;image&quot;, images[0, :, :, :])</span><br><span class="line">            print(&#39;shape:&#123;&#125;,tpye:&#123;&#125;,labels:&#123;&#125;&#39;.format(images.shape,images.dtype,labels))</span><br><span class="line"> </span><br><span class="line">        # 停止所有线程</span><br><span class="line">        coord.request_stop()</span><br><span class="line">        coord.join(threads)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    # 参数设置</span><br><span class="line"> </span><br><span class="line">    resize_height &#x3D; 224  # 指定存储图片高度</span><br><span class="line">    resize_width &#x3D; 224  # 指定存储图片宽度</span><br><span class="line">    shuffle&#x3D;True</span><br><span class="line">    log&#x3D;5</span><br><span class="line">    # 产生train.record文件</span><br><span class="line">    image_dir&#x3D;&#39;dataset&#x2F;train&#39;</span><br><span class="line">    train_labels &#x3D; &#39;dataset&#x2F;train.txt&#39;  # 图片路径</span><br><span class="line">    train_record_output &#x3D; &#39;dataset&#x2F;record&#x2F;train&#123;&#125;.tfrecords&#39;.format(resize_height)</span><br><span class="line">    create_records(image_dir,train_labels, train_record_output, resize_height, resize_width,shuffle,log)</span><br><span class="line">    train_nums&#x3D;get_example_nums(train_record_output)</span><br><span class="line">    print(&quot;save train example nums&#x3D;&#123;&#125;&quot;.format(train_nums))</span><br><span class="line"> </span><br><span class="line">    # 产生val.record文件</span><br><span class="line">    image_dir&#x3D;&#39;dataset&#x2F;val&#39;</span><br><span class="line">    val_labels &#x3D; &#39;dataset&#x2F;val.txt&#39;  # 图片路径</span><br><span class="line">    val_record_output &#x3D; &#39;dataset&#x2F;record&#x2F;val&#123;&#125;.tfrecords&#39;.format(resize_height)</span><br><span class="line">    create_records(image_dir,val_labels, val_record_output, resize_height, resize_width,shuffle,log)</span><br><span class="line">    val_nums&#x3D;get_example_nums(val_record_output)</span><br><span class="line">    print(&quot;save val example nums&#x3D;&#123;&#125;&quot;.format(val_nums))</span><br><span class="line"> </span><br><span class="line">    # 测试显示函数</span><br><span class="line">    # disp_records(train_record_output,resize_height, resize_width)</span><br><span class="line">    batch_test(train_record_output,resize_height, resize_width)</span><br></pre></td></tr></table></figure><br>5.训练模型<p></p><p><em>vgg16.py</em><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br></pre></td><td class="code"><pre><span class="line">#vgg16_train_and_val</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import pdb</span><br><span class="line">import os</span><br><span class="line">from datetime import datetime</span><br><span class="line">from create_tf_record import *</span><br><span class="line">import tensorflow.contrib.slim as slim</span><br><span class="line"> </span><br><span class="line">print(&quot;Tensorflow version:&#123;&#125;&quot;.format(tf.__version__))</span><br><span class="line">labels_nums &#x3D; 8  # 类别个数</span><br><span class="line">batch_size &#x3D; 1  #</span><br><span class="line">resize_height &#x3D; 224  # 指定存储图片高度</span><br><span class="line">resize_width &#x3D; 224  # 指定存储图片宽度</span><br><span class="line">depths &#x3D; 3</span><br><span class="line">data_shape &#x3D; [batch_size, resize_height, resize_width, depths]</span><br><span class="line"> </span><br><span class="line"># 定义input_images为图片数据</span><br><span class="line">input_images &#x3D; tf.placeholder(dtype&#x3D;tf.float32, shape&#x3D;[None, resize_height, resize_width, depths], name&#x3D;&#39;input&#39;)</span><br><span class="line"># 定义input_labels为labels数据</span><br><span class="line"># input_labels &#x3D; tf.placeholder(dtype&#x3D;tf.int32, shape&#x3D;[None], name&#x3D;&#39;label&#39;)</span><br><span class="line">input_labels &#x3D; tf.placeholder(dtype&#x3D;tf.int32, shape&#x3D;[None, labels_nums], name&#x3D;&#39;label&#39;)</span><br><span class="line"> </span><br><span class="line"># 定义dropout的概率</span><br><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32,name&#x3D;&#39;keep_prob&#39;)</span><br><span class="line">is_training &#x3D; tf.placeholder(tf.bool, name&#x3D;&#39;is_training&#39;)</span><br><span class="line">def net_evaluation(sess,loss,accuracy,val_images_batch,val_labels_batch,val_nums):</span><br><span class="line">    val_max_steps &#x3D; int(val_nums &#x2F; batch_size)</span><br><span class="line">    val_losses &#x3D; []</span><br><span class="line">    val_accs &#x3D; []</span><br><span class="line">    for _ in range(val_max_steps):</span><br><span class="line">        val_x, val_y &#x3D; sess.run([val_images_batch, val_labels_batch])</span><br><span class="line">        # print(&#39;labels:&#39;,val_y)</span><br><span class="line">        # val_loss &#x3D; sess.run(loss, feed_dict&#x3D;&#123;x: val_x, y: val_y, keep_prob: 1.0&#125;)</span><br><span class="line">        # val_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x: val_x, y: val_y, keep_prob: 1.0&#125;)</span><br><span class="line">        val_loss,val_acc &#x3D; sess.run([loss,accuracy], feed_dict&#x3D;&#123;input_images: val_x, input_labels: val_y, keep_prob:1.0, is_training: False&#125;)</span><br><span class="line">        val_losses.append(val_loss)</span><br><span class="line">        val_accs.append(val_acc)</span><br><span class="line">    mean_loss &#x3D; np.array(val_losses, dtype&#x3D;np.float32).mean()</span><br><span class="line">    mean_acc &#x3D; np.array(val_accs, dtype&#x3D;np.float32).mean()</span><br><span class="line">    return mean_loss, mean_acc</span><br><span class="line"> </span><br><span class="line">class Vgg16:</span><br><span class="line">    vgg_mean &#x3D; [103.939, 116.779, 123.68]   </span><br><span class="line"> </span><br><span class="line">    def __init__(self, vgg16_npy_path&#x3D;None,input&#x3D;None, restore_from&#x3D;None):</span><br><span class="line">        # pre-trained parameters</span><br><span class="line">        try:</span><br><span class="line">            self.data_dict &#x3D; np.load(vgg16_npy_path, encoding&#x3D;&#39;latin1&#39;).item()</span><br><span class="line">        except FileNotFoundError:</span><br><span class="line">            print(&#39;Please download VGG16 parameters from here https:&#x2F;&#x2F;mega.nz&#x2F;#!YU1FWJrA!O1ywiCS2IiOlUCtCpI6HTJOMrneN-Qdv3ywQP5poecM\nOr from my Baidu Cloud: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1Spps1Wy0bvrQHH2IMkRfpg&#39;)</span><br><span class="line"> </span><br><span class="line">        # self.tfx &#x3D; tf.placeholder(tf.float32, [None, 224, 224, 3])</span><br><span class="line">        self.sess &#x3D; tf.Session()</span><br><span class="line">        self.tfx &#x3D; input</span><br><span class="line">        self.tfy &#x3D; tf.placeholder(tf.float32, [None, 1])</span><br><span class="line"> </span><br><span class="line">        # Convert RGB to BGR</span><br><span class="line">        red, green, blue &#x3D; tf.split(axis&#x3D;3, num_or_size_splits&#x3D;3, value&#x3D;self.tfx * 255.0)</span><br><span class="line">        bgr &#x3D; tf.concat(axis&#x3D;3, values&#x3D;[</span><br><span class="line">            blue - self.vgg_mean[0],</span><br><span class="line">            green - self.vgg_mean[1],</span><br><span class="line">            red - self.vgg_mean[2],</span><br><span class="line">        ])</span><br><span class="line"> </span><br><span class="line">        # pre-trained VGG layers are fixed in fine-tune</span><br><span class="line">        conv1_1 &#x3D; self.conv_layer(bgr, &quot;conv1_1&quot;)</span><br><span class="line">        conv1_2 &#x3D; self.conv_layer(conv1_1, &quot;conv1_2&quot;)</span><br><span class="line">        pool1 &#x3D; self.max_pool(conv1_2, &#39;pool1&#39;)</span><br><span class="line"> </span><br><span class="line">        conv2_1 &#x3D; self.conv_layer(pool1, &quot;conv2_1&quot;)</span><br><span class="line">        conv2_2 &#x3D; self.conv_layer(conv2_1, &quot;conv2_2&quot;)</span><br><span class="line">        pool2 &#x3D; self.max_pool(conv2_2, &#39;pool2&#39;)</span><br><span class="line"> </span><br><span class="line">        conv3_1 &#x3D; self.conv_layer(pool2, &quot;conv3_1&quot;)</span><br><span class="line">        conv3_2 &#x3D; self.conv_layer(conv3_1, &quot;conv3_2&quot;)</span><br><span class="line">        conv3_3 &#x3D; self.conv_layer(conv3_2, &quot;conv3_3&quot;)</span><br><span class="line">        pool3 &#x3D; self.max_pool(conv3_3, &#39;pool3&#39;)</span><br><span class="line"> </span><br><span class="line">        conv4_1 &#x3D; self.conv_layer(pool3, &quot;conv4_1&quot;)</span><br><span class="line">        conv4_2 &#x3D; self.conv_layer(conv4_1, &quot;conv4_2&quot;)</span><br><span class="line">        conv4_3 &#x3D; self.conv_layer(conv4_2, &quot;conv4_3&quot;)</span><br><span class="line">        pool4 &#x3D; self.max_pool(conv4_3, &#39;pool4&#39;)</span><br><span class="line"> </span><br><span class="line">        conv5_1 &#x3D; self.conv_layer(pool4, &quot;conv5_1&quot;)</span><br><span class="line">        conv5_2 &#x3D; self.conv_layer(conv5_1, &quot;conv5_2&quot;)</span><br><span class="line">        conv5_3 &#x3D; self.conv_layer(conv5_2, &quot;conv5_3&quot;)</span><br><span class="line">        pool5 &#x3D; self.max_pool(conv5_3, &#39;pool5&#39;)</span><br><span class="line"> </span><br><span class="line">        # detach original VGG fc layers and</span><br><span class="line">        # reconstruct your own fc layers serve for your own purpose</span><br><span class="line">        pool5_shape &#x3D; pool5.get_shape().as_list()</span><br><span class="line">        nodes &#x3D; pool5_shape[1] * pool5_shape[2] * pool5_shape[3]</span><br><span class="line">        self.flatten &#x3D; tf.reshape(pool5, [-1, nodes])</span><br><span class="line">        self.fc6 &#x3D; tf.layers.dense(self.flatten, 256, tf.nn.relu, name&#x3D;&#39;fc6&#39;)</span><br><span class="line">        self.out &#x3D; tf.layers.dense(self.fc6, labels_nums, name&#x3D;&#39;out&#39;)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    def max_pool(self, bottom, name):</span><br><span class="line">        return tf.nn.max_pool(bottom, ksize&#x3D;[1, 2, 2, 1], strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;SAME&#39;, name&#x3D;name)</span><br><span class="line"> </span><br><span class="line">    def conv_layer(self, bottom, name):</span><br><span class="line">        with tf.variable_scope(name):   # CNN&#39;s filter is constant, NOT Variable that can be trained</span><br><span class="line">            conv &#x3D; tf.nn.conv2d(bottom, self.data_dict[name][0], [1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">            lout &#x3D; tf.nn.relu(tf.nn.bias_add(conv, self.data_dict[name][1]))</span><br><span class="line">            return lout</span><br><span class="line"> </span><br><span class="line">    def train(self, x, y):</span><br><span class="line">        loss, _ &#x3D; self.sess.run([self.loss, self.train_op], &#123;self.tfx: x, self.tfy: y&#125;)</span><br><span class="line">        return loss</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    def save(self, path&#x3D;&#39;.&#x2F;model&#x2F;&#39;):</span><br><span class="line">        saver &#x3D; tf.train.Saver()</span><br><span class="line">        saver.save(self.sess, path, write_meta_graph&#x3D;False)</span><br><span class="line"> </span><br><span class="line">def train(train_record_file,</span><br><span class="line">          train_log_step,</span><br><span class="line">          train_param,</span><br><span class="line">          val_record_file,</span><br><span class="line">          val_log_step,</span><br><span class="line">          labels_nums,</span><br><span class="line">          data_shape,</span><br><span class="line">          snapshot,</span><br><span class="line">          snapshot_prefix):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    :param train_record_file: 训练的tfrecord文件</span><br><span class="line">    :param train_log_step: 显示训练过程log信息间隔</span><br><span class="line">    :param train_param: train参数</span><br><span class="line">    :param val_record_file: 验证的tfrecord文件</span><br><span class="line">    :param val_log_step: 显示验证过程log信息间隔</span><br><span class="line">    :param val_param: val参数</span><br><span class="line">    :param labels_nums: labels数</span><br><span class="line">    :param data_shape: 输入数据shape</span><br><span class="line">    :param snapshot: 保存模型间隔</span><br><span class="line">    :param snapshot_prefix: 保存模型文件的前缀名</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    [base_lr,max_steps]&#x3D;train_param</span><br><span class="line">    [batch_size,resize_height,resize_width,depths]&#x3D;data_shape</span><br><span class="line"> </span><br><span class="line">    # 获得训练和测试的样本数</span><br><span class="line">    train_nums&#x3D;get_example_nums(train_record_file)</span><br><span class="line">    val_nums&#x3D;get_example_nums(val_record_file)</span><br><span class="line">    print(&#39;train nums:%d,val nums:%d&#39;%(train_nums,val_nums))</span><br><span class="line"> </span><br><span class="line">    # 从record中读取图片和labels数据</span><br><span class="line">    # train数据,训练数据一般要求打乱顺序shuffle&#x3D;True</span><br><span class="line">    train_images, train_labels &#x3D; read_records(train_record_file, resize_height, resize_width, type&#x3D;&#39;normalization&#39;)</span><br><span class="line">    train_images_batch, train_labels_batch &#x3D; get_batch_images(train_images, train_labels,</span><br><span class="line">                                                              batch_size&#x3D;batch_size, labels_nums&#x3D;labels_nums,</span><br><span class="line">                                                              one_hot&#x3D;True, shuffle&#x3D;False)</span><br><span class="line">    # val数据,验证数据可以不需要打乱数据</span><br><span class="line">    val_images, val_labels &#x3D; read_records(val_record_file, resize_height, resize_width, type&#x3D;&#39;normalization&#39;)</span><br><span class="line">    val_images_batch, val_labels_batch &#x3D; get_batch_images(val_images, val_labels,</span><br><span class="line">                                                          batch_size&#x3D;batch_size, labels_nums&#x3D;labels_nums,</span><br><span class="line">                                                          one_hot&#x3D;True, shuffle&#x3D;False)</span><br><span class="line"> </span><br><span class="line">    # Define the model:</span><br><span class="line">    # with slim.arg_scope(inception_v3.inception_v3_arg_scope()):</span><br><span class="line">    #     out, end_points &#x3D; inception_v3.inception_v3(inputs&#x3D;input_images, num_classes&#x3D;labels_nums, dropout_keep_prob&#x3D;keep_prob, is_training&#x3D;is_training)</span><br><span class="line">    vgg &#x3D; Vgg16(vgg16_npy_path&#x3D;&#39;.&#x2F;vgg16.npy&#39;,input&#x3D;input_images)</span><br><span class="line">    out &#x3D; vgg.out</span><br><span class="line">    # Specify the loss function: tf.losses定义的loss函数都会自动添加到loss函数,不需要add_loss()了</span><br><span class="line">    tf.losses.softmax_cross_entropy(onehot_labels&#x3D;input_labels, logits&#x3D;out)#添加交叉熵损失loss&#x3D;1.6</span><br><span class="line">    # slim.losses.add_loss(my_loss)</span><br><span class="line">    loss &#x3D; tf.losses.get_total_loss(add_regularization_losses&#x3D;True)#添加正则化损失loss&#x3D;2.2</span><br><span class="line">    accuracy &#x3D; tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, 1), tf.argmax(input_labels, 1)), tf.float32))</span><br><span class="line">    # Specify the optimization scheme:</span><br><span class="line">    optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate&#x3D;base_lr)</span><br><span class="line">    train_op &#x3D; slim.learning.create_train_op(total_loss&#x3D;loss,optimizer&#x3D;optimizer)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    saver &#x3D; tf.train.Saver()</span><br><span class="line">    max_acc&#x3D;0.0</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        sess.run(tf.local_variables_initializer())</span><br><span class="line">        coord &#x3D; tf.train.Coordinator()</span><br><span class="line">        threads &#x3D; tf.train.start_queue_runners(sess&#x3D;sess, coord&#x3D;coord)</span><br><span class="line">        for i in range(max_steps+1):</span><br><span class="line">            batch_input_images, batch_input_labels &#x3D; sess.run([train_images_batch, train_labels_batch])</span><br><span class="line">            _, train_loss &#x3D; sess.run([train_op, loss], feed_dict&#x3D;&#123;input_images:batch_input_images,</span><br><span class="line">                                                                      input_labels:batch_input_labels,</span><br><span class="line">                                                                      keep_prob:0.5, is_training:True&#125;)</span><br><span class="line">            # train测试(这里仅测试训练集的一个batch)</span><br><span class="line">            if i%train_log_step &#x3D;&#x3D; 0:</span><br><span class="line">                train_acc &#x3D; sess.run(accuracy, feed_dict&#x3D;&#123;input_images:batch_input_images,</span><br><span class="line">                                                          input_labels: batch_input_labels,</span><br><span class="line">                                                          keep_prob:1.0, is_training: False&#125;)</span><br><span class="line">                print(&quot;%s: Step [%d]  train Loss : %f, training accuracy :  %g&quot; % (datetime.now(), i, train_loss, train_acc))</span><br><span class="line"> </span><br><span class="line">            # val测试(测试全部val数据)</span><br><span class="line">            if i%val_log_step &#x3D;&#x3D; 0:</span><br><span class="line">                mean_loss, mean_acc&#x3D;net_evaluation(sess, loss, accuracy, val_images_batch, val_labels_batch,val_nums)</span><br><span class="line">                print(&quot;%s: Step [%d]  val Loss : %f, val accuracy :  %g&quot; % (datetime.now(), i, mean_loss, mean_acc))</span><br><span class="line"> </span><br><span class="line">            # 模型保存:每迭代snapshot次或者最后一次保存模型</span><br><span class="line">            if (i %snapshot &#x3D;&#x3D; 0 and i &gt;0)or i &#x3D;&#x3D; max_steps:</span><br><span class="line">                print(&#39;-----save:&#123;&#125;-&#123;&#125;&#39;.format(snapshot_prefix,i))</span><br><span class="line">                saver.save(sess, snapshot_prefix, global_step&#x3D;i)</span><br><span class="line">            # 保存val准确率最高的模型</span><br><span class="line">            if mean_acc&gt;max_acc and mean_acc&gt;0.5:</span><br><span class="line">                max_acc&#x3D;mean_acc</span><br><span class="line">                path &#x3D; os.path.dirname(snapshot_prefix)</span><br><span class="line">                best_models&#x3D;os.path.join(path,&#39;best_models_&#123;&#125;_&#123;:.4f&#125;.ckpt&#39;.format(i,max_acc))</span><br><span class="line">                print(&#39;------save:&#123;&#125;&#39;.format(best_models))</span><br><span class="line">                saver.save(sess, best_models)</span><br><span class="line"> </span><br><span class="line">        coord.request_stop()</span><br><span class="line">        coord.join(threads)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    train_record_file&#x3D;&#39;dataset&#x2F;record&#x2F;train224.tfrecords&#39;</span><br><span class="line">    val_record_file&#x3D;&#39;dataset&#x2F;record&#x2F;val224.tfrecords&#39;</span><br><span class="line"> </span><br><span class="line">    train_log_step&#x3D;100</span><br><span class="line">    base_lr &#x3D; 0.01  # 学习率</span><br><span class="line">    max_steps &#x3D; 200000  # 迭代次数</span><br><span class="line">    train_param&#x3D;[base_lr,max_steps]</span><br><span class="line"> </span><br><span class="line">    val_log_step&#x3D;200</span><br><span class="line">    snapshot&#x3D;2000#保存文件间隔</span><br><span class="line">    snapshot_prefix&#x3D;&#39;.&#x2F;models&#x2F;model.ckpt&#39;</span><br><span class="line">    train(train_record_file&#x3D;train_record_file,</span><br><span class="line">          train_log_step&#x3D;train_log_step,</span><br><span class="line">          train_param&#x3D;train_param,</span><br><span class="line">          val_record_file&#x3D;val_record_file,</span><br><span class="line">          val_log_step&#x3D;val_log_step,</span><br><span class="line">          labels_nums&#x3D;labels_nums,</span><br><span class="line">          data_shape&#x3D;data_shape,</span><br><span class="line">          snapshot&#x3D;snapshot,</span><br><span class="line">          snapshot_prefix&#x3D;snapshot_prefix)</span><br></pre></td></tr></table></figure><p></p><h2 id="3-结果展示"><a href="#3-结果展示" class="headerlink" title="3.结果展示"></a>3.结果展示</h2><p>用实验室服务器训练了20万代，在验证集上的准确率达到了90.75%。以下是预测结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">test_images\flower1.jpg</span><br><span class="line">test_images\flower1.jpg is: pre labels:[0],name:[&#39;flower&#39;] score: [ 1.]</span><br><span class="line">test_images\flower2.jpg</span><br><span class="line">test_images\flower2.jpg is: pre labels:[0],name:[&#39;flower&#39;] score: [ 1.]</span><br><span class="line">test_images\kittycat.jpg</span><br><span class="line">test_images\kittycat.jpg is: pre labels:[6],name:[&#39;kittycat&#39;] score: [ 0.4819051]</span><br><span class="line">test_images\kittycat2.jpg</span><br><span class="line">test_images\kittycat2.jpg is: pre labels:[6],name:[&#39;kittycat&#39;] score: [ 0.4819051]</span><br><span class="line">test_images\lion.jpg</span><br><span class="line">test_images\lion.jpg is: pre labels:[6],name:[&#39;kittycat&#39;] score: [ 0.4819051]</span><br><span class="line">test_images\plane.jpg</span><br><span class="line">test_images\plane.jpg is: pre labels:[4],name:[&#39;plane&#39;] score: [ 1.]</span><br><span class="line">test_images\plane2.jpg</span><br><span class="line">test_images\plane2.jpg is: pre labels:[1],name:[&#39;guitar&#39;] score: [ 1.]</span><br><span class="line">test_images\tiger0.jpg</span><br><span class="line">test_images\tiger0.jpg is: pre labels:[5],name:[&#39;tiger&#39;] score: [ 1.]</span><br><span class="line">test_images\tiger1.jpg</span><br><span class="line">test_images\tiger1.jpg is: pre labels:[5],name:[&#39;tiger&#39;] score: [ 1.]</span><br></pre></td></tr></table></figure><p></p></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> jackcywang</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://wangng.com/articles/64ee13ab.html/" title="基于vgg16的迁移学习,训练自己的数据集(含预测结果)">https://wangng.com/articles/64ee13ab.html/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deeplearning/" rel="tag"><i class="fa fa-tag"></i> Deeplearning</a><a href="/tags/Classification/" rel="tag"><i class="fa fa-tag"></i> Classification</a></div><div class="post-nav"><div class="post-nav-item"><a href="/articles/4a644d6f.html/" rel="prev" title="目标检测之ROI Pooling"><i class="fa fa-chevron-left"></i> 目标检测之ROI Pooling</a></div><div class="post-nav-item"> <a href="/articles/8c764809.html/" rel="next" title="目标检测之SSD解读">目标检测之SSD解读<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="tabs tabs-comment"><ul class="nav-tabs"><li class="tab"><a href="#comment-gitalk">gitalk</a></li><li class="tab"><a href="#comment-valine">valine</a></li></ul><div class="tab-content"><div class="tab-pane gitalk" id="comment-gitalk"><div class="comments" id="gitalk-container"></div></div><div class="tab-pane valine" id="comment-valine"><div class="comments" id="valine-comments"></div></div></div></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-vggNet简介"><span class="nav-number">1.</span> <span class="nav-text">1.vggNet简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-vgg16实现迁移学习"><span class="nav-number">2.</span> <span class="nav-text">2.vgg16实现迁移学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-结果展示"><span class="nav-number">3.</span> <span class="nav-text">3.结果展示</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">jackcywang</p><div class="site-description" itemprop="description">不经一番彻骨寒,怎得梅花扑鼻香</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">66</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">15</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">66</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/jackcywang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jackcywang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/weixin_42631693" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_42631693" rel="noopener" target="_blank"><i class="fa fa-fw fa-CSDN"></i> CSDN</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/hao-han-zhong-de-yipian-xie" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;hao-han-zhong-de-yipian-xie" rel="noopener" target="_blank"><i class="fa fa-fw fa-知乎"></i> 知乎</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2019 – <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">jackcywang</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span title="站点总字数">263k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:59</span></div><span id="sitetime"></span><script language="javascript">function siteTime(){window.setTimeout("siteTime()",1e3);var e=36e5,t=24*e,o=new Date,i=o.getFullYear(),n=o.getMonth()+1,a=o.getDate(),r=o.getHours(),s=o.getMinutes(),g=o.getSeconds(),l=Date.UTC(2020,1,1,15,0,0),m=Date.UTC(i,n,a,r,s,g)-l,M=Math.floor(m/t),T=Math.floor((m-M*t)/e),u=Math.floor((m-M*t-T*e)/6e4),f=Math.floor((m-M*t-T*e-6e4*u)/1e3);document.getElementById("sitetime").innerHTML=" 在此已等候"+M+" 天 "+T+" 小时 "+u+" 分钟 "+f+" 秒"}siteTime()</script><div> <a href="http://www.miitbeian.gov.cn/" rel="noopener" target="_blank">陕ICP备19023518号-1</a> <span class="post-meta-divider">|</span> <span style="padding-left:25px;background:url(/images/beian.png) no-repeat left center" rel="nofollow"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=61019002001038" rel="noopener" target="_blank">陕公网安备61019002001038号</a></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script async src="/js/cursor/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script><script>
    function timer() {
      var ages = moment.preciseDiff(moment(),moment(20191220,"YYYYMMDD"));
      //去除时分秒信息
      ages = ages.replace(/\s?\d{0,2}\s+hours?/, "");
      ages = ages.replace(/\s?\d{0,2}\s+minutes?/, "");
      ages = ages.replace(/\s?\d{0,2}\s+seconds?/, "");
      //将年月日转换为中文
      ages = ages.replace(/years?/, "年");
      ages = ages.replace(/months?/, "月");
      ages = ages.replace(/days?/, "天");
      ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
      span.innerHTML = `footer.age ${ages}`;
    }
    var span = document.createElement("span");
    //插入到agesicon之后
    var agesicon = document.querySelector(".footer-ages-icon");
    document.querySelector(".copyright").insertBefore(span, agesicon.nextSibling);
    timer();
  </script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '500094376d9d18a3ed06',
      clientSecret: '71db1de3f4d19d73479a0ad6887d90081e8b7ca1',
      repo        : 'MyGitalk',
      owner       : 'jackcywang',
      admin       : ['jackcywang'],
      id          : 'e88dd39f4d282c9ee7b04a617cc5fdd6',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script><script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qHl8YMnB0b5OHNGx0AO0fjIE-gzGzoHsz',
      appKey     : 'wN3c6hhMnkEwVWOSLF0xQ3In',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><script src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><script src="https://player.lmih.cn/player/js/player.js" id="myhk" key="158704924834" m="1"></script></body></html>
<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"wangng.com",root:"/",scheme:"Pisces",version:"7.7.2",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="论文:Fast-RCNN Fast-RCNN"><meta property="og:type" content="article"><meta property="og:title" content="目标检测之Fast-RCNN,Faster-RCNN解读"><meta property="og:url" content="https://wangng.com/articles/a76a2fca.html/index.html"><meta property="og:site_name" content="jackcywang&#39;s blog"><meta property="og:description" content="论文:Fast-RCNN Fast-RCNN"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/spp.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/spp2.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/spp3.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig1.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig3.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig2.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig4.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig5.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig6.png"><meta property="og:image" content="https://wangng.com/articles/a76a2fca.html/fig7.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig8.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig9.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig10.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig11.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig12.png"><meta property="og:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/fig13.png"><meta property="article:published_time" content="2019-05-16T13:25:46.000Z"><meta property="article:modified_time" content="2020-06-24T08:13:47.176Z"><meta property="article:author" content="jackcywang"><meta property="article:tag" content="Object Detection"><meta property="article:tag" content="Fast RCNN"><meta property="article:tag" content="Faster RCNN"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://cdn.wangng.com/deeplearning/objectdetection/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFast-RCNN,Faster-RCNN%E8%A7%A3%E8%AF%BB/spp.png"><link rel="canonical" href="https://wangng.com/articles/a76a2fca.html/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>目标检测之Fast-RCNN,Faster-RCNN解读 | jackcywang's blog</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?8ce866754ae6c95a7238894d36ad3ab0";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div> <a href="https://github.com/jackcywang" target="_blank" rel="noopener" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64ceaa;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">jackcywang's blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">梦想与现实只在一念之间</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li></ul></nav></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://wangng.com/articles/a76a2fca.html/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="jackcywang"><meta itemprop="description" content="不经一番彻骨寒,怎得梅花扑鼻香"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="jackcywang's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 目标检测之Fast-RCNN,Faster-RCNN解读</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-16 21:25:46" itemprop="dateCreated datePublished" datetime="2019-05-16T21:25:46+08:00">2019-05-16</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deeplearning/" itemprop="url" rel="index"><span itemprop="name">Deeplearning</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deeplearning/Object-Detection/" itemprop="url" rel="index"><span itemprop="name">Object Detection</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/articles/a76a2fca.html/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/articles/a76a2fca.html/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>2.6k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>2 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>论文:<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Fast-RCNN</a></p><center><font size="4.5" color="black">Fast-RCNN</font></center><a id="more"></a><h3 id="RCNN问题回顾"><a href="#RCNN问题回顾" class="headerlink" title="RCNN问题回顾"></a>RCNN问题回顾</h3><ol><li>训练要分多步,RCNN中的卷积网络需要先在一个大规模数据集先训练,然后在VOC数据集上finetune,对于SVM分类和目标框回归,都需要CNN计算的特征,而这些特征需要预先保存到磁盘中.这显然即浪费时间又浪费存储空间</li><li>预测的时候,每个proposal都需要经过CNN提取一遍特征(大约2000次),时间问题很严重.</li><li>每个proposal为了保持fc层大小固定,都需要cut或warp层固定大小,信息丢失或几何失真(CNN的卷积层不需要固定尺寸的图像，全连接层是需要固定大小输入的).</li></ol><h3 id="SppNet"><a href="#SppNet" class="headerlink" title="SppNet"></a>SppNet</h3><p>　　RCNN中每个proposal需要经过CNN重复计算,SppNet针对这个问题进行改进,每次对一整张图片用CNN提取feature map,然后在feature maps中找到各个候选框的区域。而针对每个proposal为了保持fc层大小固定,都需要cut或warp层固定大小这个问题,使用了空间金字塔池化的方法,将feature map中各个候选的目标框输出固定的大小的特征,从而可以保证可以输入任意大小的图片。但是SppNet的训练和RCNN一样也是一个多步训练的过程,这个过程也比较繁杂.<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/spp.png" alt><br>　　这里使用Conv5层的feature map,然后分成1x1，2x2，4x4三个pooling结构(这部分结构如上图所示)，对每个输入(这里每个输入大小是不一样的)都作max pooling(论文使用的)，出来的特征再连接到一起，就是(16+4+1)x256的特征向量。<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/spp2.png" alt><br>　　无论输入图像大小如何，出来的特征固定是(16+4+1)x256维度。这样就实现了不管图像中候选区域尺寸如何，SPP层的输出永远是(16+4+1)x256特征向量。SppNet的结构<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/spp3.png" alt></p><h3 id="模型改进"><a href="#模型改进" class="headerlink" title="模型改进"></a>模型改进</h3><p>　　针对RCNN和SppNet的问题,Fast-RCNN做了以下改进措施:</p><ol><li><p>针对RCNN中每个region需要被CNN提取特征的问题,借鉴SppNet的方法,先对整张图片提取feature map,再将该图片用Selective Search算法提取到2000个region proposal映射到pool5层的feature map中找到对应的proposal feature map;</p></li><li><p>针对RCNN需要输入固定大小的region proposal问题,解决方法是将1中每个proposal feature map用ROI Pooling层输出成固定大小的$H\times W$(7x7)的ROI feature map.再送到全连接层</p></li><li><p>针对需要多阶段训练的问题,将SVM替换成softmax多分类函数,同时提出了多任务损失函数(Multi-task Loss)，将边框回归直接加入到CNN网络中训练，同时包含了候选区域分类损失和位置回归损失。这样整个的训练过程只包含提取候选区域和CNN训练两个阶段。此外，Fast R-CNN在网络微调的过程中，不仅微调全连接层，对部分卷积层也进行了微调，得到了更好的检测结果。</p></li></ol><p>Fast-RCNN的整个流程如下:<br>(1). 输入一张待检测图像；<br>(2). 提取候选区域：利用Selective Search算法在输入图像中提取出候选区域，并把这些候选区域按照空间位置关系映射到最后的卷积特征层；<br>(3). 区域归一化：对于卷积特征层上的每个候选区域进行<a href="https://www.wangng.com/articles/4a644d6f.html/" target="_blank" rel="noopener">RoI Pooling</a>操作，得到固定维度的特征；<br>(4). 分类与回归：将提取到的特征输入全连接层，然后用Softmax进行分类，对候选区域的位置进行回归。<br>Fast R-CNN尽管速度和精度上都有了很大的提升，但仍然未能实现端到端(end-to-end)的目标检测，比如候选区域的获得不能同步进行，速度上还有提升空间。<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig1.png" alt><br>实际的训练和测试过程如下</p><table><tr><td><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig3.png" border="0"></td><td><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig2.png" border="0"></td></tr></table><p>　　在实际训练中，每个mini-batch包含2张图像和128个region proposal(ROI),也就是说每张图像有64个ROI。然后从这些ROI中挑选约25%的ROI，这些ROI和ground truth的IOU值都大于0.5。另外只采用随机水平翻转的方式增加数据集。测试的时候则每张图像大约2000个ROI。</p><h3 id="多任务损失"><a href="#多任务损失" class="headerlink" title="多任务损失"></a>多任务损失</h3><p>　　Fast RCNN在输出层有两个分支,第一个分支是分类,输出每个ROI的类别概率,共有K+1类,K个类别和1个背景,第二个分支输出目标框的偏置。训练的时候Fast RCNN将分类loss和回归loss整合在一起，其中分类采用log loss，即对真实分类的概率取负log,回归loss采用smoothl1 loss。总损失为:</p><script type="math/tex;mode=display">
L\left(p, u, t^{u}, v\right)=L_{\mathrm{cls}}(p, u)+\lambda[u \geq 1] L_{\mathrm{loc}}\left(t^{u}, v\right)\tag{1}</script><p>　　每个roi有一个真实的标记,类别为$u$,ground truth为$v$,上式中$L<em>{\mathrm{cls}}(p, u)=-logp</em>{u}$是是K+1类中预测真实类别为第$u$类的损失,而$L<em>{\mathrm{loc}}\left(t^{u}, v\right)$类别为u的预测框和真实框的损失,预测值为$t^{u}=\left(t</em>{\mathrm{x}}^{u}, t<em>{\mathrm{y}}^{u}, t</em>{\mathrm{w}}^{u}, t<em>{\mathrm{h}}^{u}\right)$,真实值为$v=\left(v</em>{\mathbf{x}}, \boldsymbol{v}<em>{\mathbf{y}}, \boldsymbol{v}</em>{\mathbf{w}}, \boldsymbol{v}_{\mathbf{h}}\right)$,$[u \geq 1]$表示正例为1,背景时为0。回归损失定义如下</p><script type="math/tex;mode=display">
L_{\mathrm{loc}}\left(t^{u}, v\right)=\sum_{i \in\{\mathrm{x}, \mathrm{y}, \mathrm{w}, \mathrm{h}\}} \operatorname{smooth}_{L_{1}}\left(t_{i}^{u}-v_{i}\right)\tag{2}</script><p>其中SmoothL1函数定义为</p><script type="math/tex;mode=display">
\operatorname{smooth}_{L_{1}}(x)=\left\{\begin{array}{ll}
0.5 x^{2} & \text { if }|x|<1 \\
|x|-0.5 & \text { otherwise }\tag{3}
\end{array}\right.</script><p>L1损失对于外点(outlier)的敏感度的低于L2损失,当回归目标是没有边界的时候,训练L2损失需要小心的调整学习率才能保证阻止梯度爆炸.</p><hr><center><font size="4.5" color="black">Faster-RCNN</font></center><p>论文:<a href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="noopener">Faster-RCNN</a></p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>　　Faster-RCNN是在Fast-RCNN的基础上改进而来的,本质是解决Fast-RCNN中region proposal需要单独使用区域搜索算法搜索出来的问题,这里作者提出使用RPN网络搜索出region proposal,再用Fast RCNN的head来分类回归,是RPN网络和Fast-RCNN的结合。</p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig4.png" alt><br>包含了四个部分,这四个部分的功能分别是:<br>1)、Conv layers提取特征图：<br>　　作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取input image的feature maps,该feature maps会用于后续的RPN层和全连接层</p><p>2)、RPN(Region Proposal Networks):<br>　　RPN网络主要用于生成region proposals，首先生成一堆Anchor box，对其进行裁剪过滤后通过softmax判断anchors属于前景(foreground)或者后景(background)，即是物体or不是物体，所以这是一个二分类；同时，另一分支bounding box regression修正anchor box，形成较精确的proposal（注：这里的较精确是相对于后面全连接层的再一次box regression而言）</p><p>3)、Roi Pooling：<br>　　该层利用RPN生成的proposals和VGG16最后一层得到的feature map，得到固定大小的proposal feature map,进入到后面可利用全连接操作来进行目标识别和定位</p><p>4)、Classifier：<br>　　会将Roi Pooling层形成固定大小的feature map进行全连接操作，利用Softmax进行具体类别的分类，同时，利用L1 Loss完成bounding box regression回归操作获得物体的精确位置.</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig5.png" alt></p><h4 id="1-Conv-Layer"><a href="#1-Conv-Layer" class="headerlink" title="1.Conv Layer"></a>1.Conv Layer</h4><p>　　Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。在Conv layers中:<br>所有的conv层都是：kernel_size=3，pad=1，stride=1<br>所有的pooling层都是：kernel_size=2,pad=1,stride=1,kernel_size=2,pad=0,stride =2.</p><p>这可以保证输出图片是输入图片的1/2.经过Conv Layer后,输出特征图分辨率为(W/16,H/16),原文是输入为(1000,600),那么输出为(60,40),最后的特征图大小是60x40x512(VGG16最后卷积层输出通道数为512)。</p><h4 id="2-RPN"><a href="#2-RPN" class="headerlink" title="2.RPN"></a>2.RPN</h4><p>　　再Faster-RCNN提出前,region proposal的获取要么使用滑动窗口法,要么使用图像分割的区域建议搜索算法(如selective search),这些方法都比较耗时,Faster-RCNN使用神经网络自己生成,如下图就是其网络结构<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig6.png" alt>　<br>如上图中标识：<br>rpn_cls：60x40x512 1x1x512x18 =&gt; 60x40x9x2<br>逐像素对其9个Anchor box进行二分类(postive/negtive)<br>rpn_bbox：60x40x512 1x1x512x36 =&gt;60x40x9x4<br>逐像素得到其9个Anchor box四个坐标信息(x,y,w,h)</p><p><img src="/articles/a76a2fca.html/fig7.png" alt>　　</p><h5 id="2-1-anchors"><a href="#2-1-anchors" class="headerlink" title="2.1.anchors"></a>2.1.anchors</h5><p>　　前面提到经过Conv layers后，图片大小变成了原来的1/16，令feat_stride=16，在生成Anchors时，我们先定义一个base_anchor，大小为16x16的box(因为特征图(60x40)上的一个点，可以对应到原图（1000x600）上一个16x16大小的区域)，源码中转化为[0,0,15,15]的数组，参数ratios=[0.5, 1, 2]scales=[8, 16, 32]<br>先看[0,0,15,15],面积保持不变，长、宽比分别为[0.5, 1, 2]是产生的Anchors box<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig8.png" alt>　<br>如果经过scales变化，即长、宽分别均为 (16x8=128)、(16x16=256)、(16x32=512)，对应anchor box如图<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig9.png" alt><br>综合以上两种变换，最后生成9个Anchor box<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig10.png" alt></p><h5 id="2-2-RPN工作原理"><a href="#2-2-RPN工作原理" class="headerlink" title="2.2 RPN工作原理"></a>2.2 RPN工作原理</h5><p><strong>流程图</strong><br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig11.png" alt></p><p><strong>rpn-data</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;  </span><br><span class="line">      name: &#39;rpn-data&#39;  </span><br><span class="line">      type: &#39;Python&#39;  </span><br><span class="line">      bottom: &#39;rpn_cls_score&#39;   #仅提供特征图的height和width的参数大小</span><br><span class="line">      bottom: &#39;gt_boxes&#39;        #ground truth box</span><br><span class="line">      bottom: &#39;im_info&#39;         #包含图片大小和缩放比例，可供过滤anchor box</span><br><span class="line">      bottom: &#39;data&#39;  </span><br><span class="line">      top: &#39;rpn_labels&#39;  </span><br><span class="line">      top: &#39;rpn_bbox_targets&#39;  </span><br><span class="line">     top: &#39;rpn_bbox_inside_weights&#39;  </span><br><span class="line">     top: &#39;rpn_bbox_outside_weights&#39;  </span><br><span class="line">     python_param &#123;  </span><br><span class="line">        module: &#39;rpn.anchor_target_layer&#39;  </span><br><span class="line">        layer: &#39;AnchorTargetLayer&#39;  </span><br><span class="line">        param_str: &quot;&#39;feat_stride&#39;: 16 \n&#39;scales&#39;: !!python&#x2F;tuple [8, 16, 32]&quot;  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><br>　　这一层主要是为特征图60<em>40上的每个像素生成9个Anchor box，并且对生成的Anchor box进行过滤和标记，参照源码，过滤和标记规则如下：<br>①去除掉超过1000</em>600这原图的边界的anchor box<br>②如果anchor box与ground truth的IoU值最大，标记为正样本，label=1<br>③如果anchor box与ground truth的IoU&gt;0.7，标记为正样本，label=1<br>④如果anchor box与ground truth的IoU&lt;0.3，标记为负样本，label=0<br>剩下的既不是正样本也不是负样本，不用于最终训练，label=-1<p></p><p><strong>Proposal</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;  </span><br><span class="line">      name: &#39;proposal&#39;  </span><br><span class="line">      type: &#39;Python&#39;  </span><br><span class="line">      bottom: &#39;rpn_cls_prob_reshape&#39; #[1,18,40,60]&#x3D;&#x3D;&gt; [batch_size, channel，height，width]Caffe的数据格式anchor box分类的概率</span><br><span class="line">      bottom: &#39;rpn_bbox_pred&#39;  # 记录训练好的四个回归值△x, △y, △w, △h</span><br><span class="line">      bottom: &#39;im_info&#39;  </span><br><span class="line">      top: &#39;rpn_rois&#39;  </span><br><span class="line">      python_param &#123;  </span><br><span class="line">        module: &#39;rpn.proposal_layer&#39;  </span><br><span class="line">        layer: &#39;ProposalLayer&#39;  </span><br><span class="line">        param_str: &quot;&#39;feat_stride&#39;: 16 \n&#39;scales&#39;: !!python&#x2F;tuple [4, 8, 16, 32]&quot;</span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><br>　　rpn_bbox_pred会记录$\Delta$x,$\Delta$y,$\Delta$w,$\Delta$h,而Conv5会生成60x40x9个anchor box，然后累加上训练好的$\Delta$x,$\Delta$y,$\Delta$w,$\Delta$h,从而得到了相较于之前更加准确的预测框region proposal，进一步对预测框进行越界剔除和使用nms非最大值抑制，剔除掉重叠的框；比如，设定IoU为0.7的阈值，即仅保留覆盖率不超过0.7的局部最大分数的box(粗筛)。最后留下大约2000个anchor(训练),然后再取前N个box(比如300个)(预测),这样，进入到下一层ROI Pooling时region proposal大约只有300个。<br>RPN网络结构总结<br>生成anchors -&gt;标记数据生成rpn-data-&gt; softmax分类器提取前景anchors-&gt; bbox reg回归前景anchors-&gt;使用nms剔除部分重叠anchor-&gt; Proposal Layer生成proposals<p></p><blockquote><p>用下图一个案例来对NMS算法进行简单介绍<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig12.png" alt><br>如上图所示，一共有6个识别为人的框，每一个框有一个置信率。<br>现在需要消除多余的:<br>·按置信率排序: 0.95, 0.9, 0.9, 0.8, 0.7, 0.7<br>·取最大0.95的框为一个物体框<br>·剩余5个框中，去掉与0.95框重叠率IoU大于0.7(可以另行设置)，则保留0.9, 0.8, 0.7三个框<br>·重复上面的步骤，直到没有框了，0.9为一个框<br>·选出来的为: 0.95, 0.9</p></blockquote><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>　　Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框<br><img src="http://cdn.wangng.com/deeplearning/objectdetection/目标检测之Fast-RCNN,Faster-RCNN解读/fig13.png" alt><br>从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：<br>1.通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了<br>2.再次对proposals进行bounding box regression，获取更高精度的rect box</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf</a><br><a href="https://blog.csdn.net/fengbingchun/article/details/87091740" target="_blank" rel="noopener">目标检测算法Fast R-CNN简介</a><br><a href="https://www.cnblogs.com/wangyong/p/8513563.html" target="_blank" rel="noopener">Faster RCNN 学习笔记</a><br><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">一文读懂Faster RCNN</a></p></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> jackcywang</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://wangng.com/articles/a76a2fca.html/" title="目标检测之Fast-RCNN,Faster-RCNN解读">https://wangng.com/articles/a76a2fca.html/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Object-Detection/" rel="tag"><i class="fa fa-tag"></i> Object Detection</a><a href="/tags/Fast-RCNN/" rel="tag"><i class="fa fa-tag"></i> Fast RCNN</a><a href="/tags/Faster-RCNN/" rel="tag"><i class="fa fa-tag"></i> Faster RCNN</a></div><div class="post-nav"><div class="post-nav-item"><a href="/articles/71e282d7.html/" rel="prev" title="目标检测之Map计算详解"><i class="fa fa-chevron-left"></i> 目标检测之Map计算详解</a></div><div class="post-nav-item"> <a href="/articles/4a644d6f.html/" rel="next" title="目标检测之ROI Pooling">目标检测之ROI Pooling<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="tabs tabs-comment"><ul class="nav-tabs"><li class="tab"><a href="#comment-gitalk">gitalk</a></li><li class="tab"><a href="#comment-valine">valine</a></li></ul><div class="tab-content"><div class="tab-pane gitalk" id="comment-gitalk"><div class="comments" id="gitalk-container"></div></div><div class="tab-pane valine" id="comment-valine"><div class="comments" id="valine-comments"></div></div></div></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#RCNN问题回顾"><span class="nav-number">1.</span> <span class="nav-text">RCNN问题回顾</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SppNet"><span class="nav-number">2.</span> <span class="nav-text">SppNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型改进"><span class="nav-number">3.</span> <span class="nav-text">模型改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多任务损失"><span class="nav-number">4.</span> <span class="nav-text">多任务损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简介"><span class="nav-number">5.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#整体框架"><span class="nav-number">6.</span> <span class="nav-text">整体框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络结构"><span class="nav-number">7.</span> <span class="nav-text">网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Conv-Layer"><span class="nav-number">7.1.</span> <span class="nav-text">1.Conv Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-RPN"><span class="nav-number">7.2.</span> <span class="nav-text">2.RPN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-anchors"><span class="nav-number">7.2.1.</span> <span class="nav-text">2.1.anchors</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-RPN工作原理"><span class="nav-number">7.2.2.</span> <span class="nav-text">2.2 RPN工作原理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification"><span class="nav-number">8.</span> <span class="nav-text">Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考"><span class="nav-number">9.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">jackcywang</p><div class="site-description" itemprop="description">不经一番彻骨寒,怎得梅花扑鼻香</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">66</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">15</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">65</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/jackcywang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jackcywang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/weixin_42631693" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_42631693" rel="noopener" target="_blank"><i class="fa fa-fw fa-CSDN"></i> CSDN</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/hao-han-zhong-de-yipian-xie" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;hao-han-zhong-de-yipian-xie" rel="noopener" target="_blank"><i class="fa fa-fw fa-知乎"></i> 知乎</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2019 – <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">jackcywang</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span title="站点总字数">261k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:57</span></div><span id="sitetime"></span><script language="javascript">function siteTime(){window.setTimeout("siteTime()",1e3);var e=36e5,t=24*e,o=new Date,i=o.getFullYear(),n=o.getMonth()+1,a=o.getDate(),r=o.getHours(),s=o.getMinutes(),g=o.getSeconds(),l=Date.UTC(2020,1,1,15,0,0),m=Date.UTC(i,n,a,r,s,g)-l,M=Math.floor(m/t),T=Math.floor((m-M*t)/e),u=Math.floor((m-M*t-T*e)/6e4),f=Math.floor((m-M*t-T*e-6e4*u)/1e3);document.getElementById("sitetime").innerHTML=" 在此已等候"+M+" 天 "+T+" 小时 "+u+" 分钟 "+f+" 秒"}siteTime()</script><div> <a href="http://www.miitbeian.gov.cn/" rel="noopener" target="_blank">陕ICP备19023518号-1</a> <span class="post-meta-divider">|</span> <span style="padding-left:25px;background:url(/images/beian.png) no-repeat left center" rel="nofollow"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=61019002001038" rel="noopener" target="_blank">陕公网安备61019002001038号</a></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><script async src="/js/cursor/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script><script>
    function timer() {
      var ages = moment.preciseDiff(moment(),moment(20191220,"YYYYMMDD"));
      //去除时分秒信息
      ages = ages.replace(/\s?\d{0,2}\s+hours?/, "");
      ages = ages.replace(/\s?\d{0,2}\s+minutes?/, "");
      ages = ages.replace(/\s?\d{0,2}\s+seconds?/, "");
      //将年月日转换为中文
      ages = ages.replace(/years?/, "年");
      ages = ages.replace(/months?/, "月");
      ages = ages.replace(/days?/, "天");
      ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
      span.innerHTML = `footer.age ${ages}`;
    }
    var span = document.createElement("span");
    //插入到agesicon之后
    var agesicon = document.querySelector(".footer-ages-icon");
    document.querySelector(".copyright").insertBefore(span, agesicon.nextSibling);
    timer();
  </script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '500094376d9d18a3ed06',
      clientSecret: '71db1de3f4d19d73479a0ad6887d90081e8b7ca1',
      repo        : 'MyGitalk',
      owner       : 'jackcywang',
      admin       : ['jackcywang'],
      id          : '36fe98546fffd6290e68a9b60f3677be',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script><script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qHl8YMnB0b5OHNGx0AO0fjIE-gzGzoHsz',
      appKey     : 'wN3c6hhMnkEwVWOSLF0xQ3In',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><script src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><script src="https://player.lmih.cn/player/js/player.js" id="myhk" key="158704924834" m="1"></script></body></html>